# -*- coding: utf-8 -*-
"""crop_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E7cpOqj4rA900Gx8Ov2ykvKC0PQgQWrt
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Load the dataset
df = pd.read_csv("crop_price_prediction_data.csv")

#Fill or drop missing values
df.dropna(inplace=True)

#Convert price to category labels (Low, Medium, High)
df['Price Category'] = pd.qcut(df['price(ton)'], q=3, labels=["Low", "Medium", "High"])

#Encode categorical columns
categorical_cols = ['State', 'City', 'Crop Type', 'seasonality']
# Check if all categorical columns exist in the DataFrame
for col in categorical_cols:
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found in DataFrame. Skipping...")
        continue  # Skip to the next column if not found
    # Create a new column with the encoded values to avoid modifying the original column
    df[col + '_encoded'] = LabelEncoder().fit_transform(df[col])

# Features and target
# Include the encoded columns instead of the original categorical columns
X = df.drop(columns=['Date', 'price(ton)', 'Price Category', 'State', 'City', 'Crop Type', 'Season'])
y = df['Price Category']

#Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC()
}

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print(classification_report(y_test, y_pred))

#Deep learning Model
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Replace 'seasonality' with the actual column name in your DataFrame, for instance, 'Season'
categorical_cols = ['State', 'City', 'Crop Type', 'Season']
for col in categorical_cols:
    if col not in df.columns:
        print(f"Warning: Column '{col}' not found in DataFrame. Skipping...")
        continue
    df[col] = LabelEncoder().fit_transform(df[col])

X = df.drop(columns=['Date', 'price(ton)', 'Price Category'])
y = df['Price Category']

# Normalize
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encode target for DNN
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.2, random_state=42)

# Deep Neural Network
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(3, activation='softmax'))  # 3 classes

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)

# Evaluation
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Deep Learning Model Accuracy: {accuracy:.4f}")

# Classification Report
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))































































pip install streamlit shap scikit-learn pandas matplotlib



















import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("crop_price_prediction_data.csv")

# Encode categorical features
label_cols = ['State', 'City', 'Crop Type', 'Month', 'Season']
df[label_cols] = df[label_cols].apply(LabelEncoder().fit_transform)

# Convert 'price' into categories (low, medium, high)
df['price_category'] = pd.qcut(df['price(ton)'], q=3, labels=['Low', 'Medium', 'High'])

# Define features and target
X = df.drop(columns=['Date', 'price(ton)', 'price_category'])
y = df['price_category']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC()
}

# Train and evaluate
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    print(f"------ {name} ------")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

from sklearn.model_selection import train_test_split, GridSearchCV

# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr_params = {'C': [0.1, 1, 10]}
lr_grid = GridSearchCV(lr, lr_params, cv=3)
lr_grid.fit(X_train_scaled, y_train)

# Decision Tree
dt = DecisionTreeClassifier()
dt_params = {'max_depth': [5, 10, 20]}
dt_grid = GridSearchCV(dt, dt_params, cv=3)
dt_grid.fit(X_train, y_train)

# Random Forest
rf = RandomForestClassifier()
rf_params = {'n_estimators': [50, 100], 'max_depth': [10, 20]}
rf_grid = GridSearchCV(rf, rf_params, cv=3)
rf_grid.fit(X_train, y_train)

# SVM
svm = SVC()
svm_params = {'C': [1, 10], 'kernel': ['rbf', 'linear']}
svm_grid = GridSearchCV(svm, svm_params, cv=3)
svm_grid.fit(X_train_scaled, y_train)

models = {
    "Logistic Regression": lr_grid,
    "Decision Tree": dt_grid,
    "Random Forest": rf_grid,
    "SVM": svm_grid
}

for name, model in models.items():
    y_pred = model.predict(X_test_scaled if "SVM" in name or "Logistic" in name else X_test)
    print(f"------ {name} ------")
    print("Best Params:", model.best_params_)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Save the best-performing model
joblib.dump(rf_grid.best_estimator_, "crop_price_model.pkl")

# Save the feature scaler
joblib.dump(scaler, "scaler.pkl")

from google.colab import files
files.download('crop_price_model.pkl')
files.download('scaler.pkl')

# Save the best-performing model
joblib.dump(rf_grid.best_estimator_, "crop_price_model.pkl")

files.download('crop_price_model.pkl')















import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, mean_squared_error

# Load dataset
df = pd.read_csv('crop_price_prediction_data.csv')

# --- Basic preprocessing ---
# Drop rows with missing values (adjust as needed)
df = df.dropna()

# Automatically detect target column (assume it's the last one)
target_column = df.columns[-1]
X = df.drop(columns=[target_column])
y = df[target_column]

# Encode categorical features (if any)
for col in X.select_dtypes(include=['object']).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Encode target if classification
is_classification = y.dtype == 'object' or len(y.unique()) <= 20
if is_classification:
    y = LabelEncoder().fit_transform(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
if is_classification:
    model = DecisionTreeClassifier(max_depth=4, random_state=42)
else:
    model = DecisionTreeRegressor(max_depth=4, random_state=42)

model.fit(X_train, y_train)

# Evaluate
if is_classification:
    y_pred = model.predict(X_test)
    print("Classification Report:\n", classification_report(y_test, y_pred))
else:
    y_pred = model.predict(X_test)
    print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))

# SHAP Explainability
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test, feature_names=X.columns.tolist())

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, mean_squared_error

# Load dataset
df = pd.read_csv('crop_price_prediction_data.csv')

# --- Basic preprocessing ---
df = df.dropna()  # Drop missing values (adjust if needed)

# Set target column (CHANGE THIS IF NEEDED)
target_column = df.columns[-1]  # Automatically choose the last column
X = df.drop(columns=[target_column])
y = df[target_column]

# Encode categorical features
for col in X.select_dtypes(include='object').columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Detect task type
is_classification = y.dtype == 'object' or len(y.unique()) <= 20
if is_classification:
    y = LabelEncoder().fit_transform(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest model
if is_classification:
    model = RandomForestClassifier(n_estimators=100, random_state=42)
else:
    model = RandomForestRegressor(n_estimators=100, random_state=42)

model.fit(X_train, y_train)

# Evaluate
if is_classification:
    y_pred = model.predict(X_test)
    print("Classification Report:\n", classification_report(y_test, y_pred))
else:
    y_pred = model.predict(X_test)
    print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))

# SHAP Explainability
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# Plot global feature importance
shap.summary_plot(shap_values, X_test, feature_names=X.columns.tolist())

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC, SVR
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, mean_squared_error

# Load the dataset
df = pd.read_csv('crop_price_prediction_data.csv')
df = df.dropna()

# Set the target column (change if needed)
target_column = df.columns[-1]
X = df.drop(columns=[target_column])
y = df[target_column]

# Encode categorical features
for col in X.select_dtypes(include='object').columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Encode target if classification
is_classification = y.dtype == 'object' or len(y.unique()) <= 20
if is_classification:
    y = LabelEncoder().fit_transform(y)

# Scale features (important for SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train SVM model
if is_classification:
    model = SVC(probability=True, kernel='rbf')
else:
    model = SVR(kernel='rbf')

model.fit(X_train, y_train)

# Evaluate
if is_classification:
    y_pred = model.predict(X_test)
    print("Classification Report:\n", classification_report(y_test, y_pred))
else:
    y_pred = model.predict(X_test)
    print("RMSE:", mean_squared_error(y_test, y_pred, squared=False))

# SHAP with KernelExplainer (slow but works for any model)
explainer = shap.KernelExplainer(model.predict, X_train[:100])  # Use a small background set
shap_values = explainer.shap_values(X_test[:10])  # Explain first 10 test samples for speed

# Plot summary
shap.summary_plot(shap_values, X_test[:10], feature_names=X.columns.tolist())

# Force plot for a single instance
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_test[0], feature_names=X.columns.tolist())

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report

# Load data
df = pd.read_csv('crop_price_prediction_data.csv')
df = df.dropna()

# Set target column (change if needed)
target_column = df.columns[-1]
X = df.drop(columns=[target_column])
y = df[target_column]

# Encode categorical features
for col in X.select_dtypes(include='object').columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Encode target labels
y = LabelEncoder().fit_transform(y)

# Scale features (important for logistic regression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train Logistic Regression
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print("Classification Report:\n", classification_report(y_test, y_pred))

# SHAP with LinearExplainer
explainer = shap.Explainer(model, X_train)
shap_values = explainer(X_test)

# Summary plot of feature importance
shap.summary_plot(shap_values, X_test, feature_names=X.columns.tolist())







